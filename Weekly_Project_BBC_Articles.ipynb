{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Weekly-Project-BBC Articles.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8btHNo7H5Cf",
        "colab_type": "text"
      },
      "source": [
        "# Organize ML projects with Scikit-Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yK9CuWlH5Ch",
        "colab_type": "text"
      },
      "source": [
        "While Machine Learning is powerful, people often overestimate it: apply machine learning to your project, and all your problems will be solved. In reality, it's not this simple. To be effective, one needs to organize the work very well. In this notebook, we will walkthrough practical aspects of a ML project. To look at the big picture, let's start with a checklist below. It should work reasonably well for most ML projects, but make sure to adapt it to your needs:\n",
        "\n",
        "1. **Define the scope of work and objective**\n",
        "    * How is your solution be used?\n",
        "    * How should performance be measured? Are there any contraints?\n",
        "    * How would the problem be solved manually?\n",
        "    * List the available assumptions, and verify if possible.\n",
        "    \n",
        "    \n",
        "2. **Get the data**\n",
        "    * Document where you can get that data\n",
        "    * Store data in a workspace you can easily access\n",
        "    * Convert the data to a format you can easily manipulate\n",
        "    * Check the overview (size, type, sample, description, statistics)\n",
        "    * Data cleaning\n",
        "    \n",
        "    \n",
        "3. **EDA & Data transformation**\n",
        "    * Study each attribute and its characteristics (missing values, type of distribution, usefulness)\n",
        "    * Visualize the data\n",
        "    * Study the correlations between attributes\n",
        "    * Feature selection, Feature Engineering, Feature scaling\n",
        "    * Write functions for all data transformations\n",
        "    \n",
        "    \n",
        "4. **Train models**\n",
        "    * Automate as much as possible\n",
        "    * Train promising models quickly using standard parameters. Measure and compare their performance\n",
        "    * Analyze the errors the models make\n",
        "    * Shortlist the top three of five most promising models, preferring models that make different types of errors.\n",
        "\n",
        "\n",
        "5. **Fine-tunning**\n",
        "    * Treat data transformation choices as hyperparameters, expecially when you are not sure about them (e.g., replace missing values with zeros or with the median value)\n",
        "    * Unless there are very few hyperparameter value to explore, prefer random search over grid search.\n",
        "    * Try ensemble methods\n",
        "    * Test your final model on the test set to estimate the generalizaiton error. Don't tweak your model again, you would start overfitting the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofeuKevOH5Ch",
        "colab_type": "text"
      },
      "source": [
        "## Example: Articles categorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2NSUqUEH5Ci",
        "colab_type": "text"
      },
      "source": [
        "### Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GttlMG-H5Cj",
        "colab_type": "text"
      },
      "source": [
        "Build a model to determine the categories of articles. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwbjWOG1H5Ck",
        "colab_type": "text"
      },
      "source": [
        "### Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWq7xex_H5Ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9W7Hzt2H5Cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bbc = pd.read_csv('https://raw.githubusercontent.com/dhminh1024/practice_datasets/master/bbc-text.csv')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teb1QvD1H5Cs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "060da6e0-f8d3-425d-b1c3-bcf2b7c6066d"
      },
      "source": [
        "bbc.sample(5)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1423</th>\n",
              "      <td>sport</td>\n",
              "      <td>robben and cole earn chelsea win cheslea salva...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1235</th>\n",
              "      <td>business</td>\n",
              "      <td>us to probe airline travel chaos the us govern...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>business</td>\n",
              "      <td>singapore growth at 8.1% in 2004 singapore s e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1185</th>\n",
              "      <td>tech</td>\n",
              "      <td>rich pickings for hi-tech thieves viruses  tro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>rap feud in 50 cent s g-unit crew us rap star ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           category                                               text\n",
              "1423          sport  robben and cole earn chelsea win cheslea salva...\n",
              "1235       business  us to probe airline travel chaos the us govern...\n",
              "71         business  singapore growth at 8.1% in 2004 singapore s e...\n",
              "1185           tech  rich pickings for hi-tech thieves viruses  tro...\n",
              "478   entertainment  rap feud in 50 cent s g-unit crew us rap star ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBW_Sg2RH5Cy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "1682cef7-84a7-49b9-8127-de697875f110"
      },
      "source": [
        "bbc.info()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2225 entries, 0 to 2224\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   category  2225 non-null   object\n",
            " 1   text      2225 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 34.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh3VRY5Zxmw6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = bbc['text']\n",
        "y = bbc['category']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svjpStyFespa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "0ba7dbcf-a211-411f-8fa0-5bd1b3f15663"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "pipeline = Pipeline([('tfidf', TfidfVectorizer(\n",
        "                        input='array',\n",
        "                        norm='l2',\n",
        "                        max_features=None,\n",
        "                        sublinear_tf=True,\n",
        "                        stop_words='english')),\n",
        "                    ('clf', SVC(\n",
        "                        C=10,\n",
        "                        kernel=\"rbf\",\n",
        "                        gamma=0.1,\n",
        "                        probability=True,\n",
        "                        class_weight=None,))])\n",
        "pipeline.fit(X_train, y_train)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='array',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words='english', strip_accents=None,\n",
              "                                 sublinear_tf=True,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('clf',\n",
              "                 SVC(C=10, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma=0.1, kernel='rbf', max_iter=-1, probability=True,\n",
              "                     random_state=None, shrinking=True, tol=0.001,\n",
              "                     verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUSTZfQElRsd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "2e66341c-9ab0-4934-9c46-95e4196a977e"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "predictions = pipeline.predict(X_test)\n",
        "print('accuracy:',accuracy_score(y_test,predictions))\n",
        "print('classification report:\\n',classification_report(y_test,predictions))\n",
        "print('confusion matrix:\\n',confusion_matrix(y_test,predictions))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.9730337078651685\n",
            "classification report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     business       0.95      0.97      0.96       106\n",
            "entertainment       0.99      0.99      0.99        87\n",
            "     politics       0.96      0.95      0.96        81\n",
            "        sport       0.99      0.98      0.98        94\n",
            "         tech       0.97      0.97      0.97        77\n",
            "\n",
            "     accuracy                           0.97       445\n",
            "    macro avg       0.97      0.97      0.97       445\n",
            " weighted avg       0.97      0.97      0.97       445\n",
            "\n",
            "confusion matrix:\n",
            " [[103   0   1   0   2]\n",
            " [  0  86   1   0   0]\n",
            " [  3   1  77   0   0]\n",
            " [  1   0   1  92   0]\n",
            " [  1   0   0   1  75]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}